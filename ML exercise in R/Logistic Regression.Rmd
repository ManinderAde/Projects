---
title: "10.3"
author: "Maninder Ade"
date: "03/09/2021"
output: pdf_document
---

#############################################
rm(list = ls())
set.seed(1)

#############################################

# Load Data from the working directory
data <- read.table("germancredit.txt", sep=" ")

head(data)

## Converting Response Variable to 0 and 1
data$V21[data$V21==1] <- 0
data$V21[data$V21==2] <- 1

## Split data to training and test sets (70% / 30%)
train <- sample(1:nrow(data), size=round(nrow(data)*0.7),replace = F)
data_train <- data[train,]
data_test <- data[-train,]

table(data_train$V21)

#  0   1 
# 492 208 

table(data_test$V21)

#  0   1 
# 208  92 

## Running Logistic Regression Model using glm
model1 <- glm(V21~., family = binomial(link = "logit"),data = data_train)
summary(model1)

#Call:
#glm(formula = V21 ~ ., family = binomial(link = "logit"), data = data_train)

# ------Deviance Residuals:
# ------    Min       1Q   Median       3Q      Max
# -------2.4438  -0.6861  -0.3608   0.6750   2.4540

# ------Coefficients:
# ------              Estimate Std. Error z value Pr(>|z|)
# ------(Intercept)  3.823e-01  1.332e+00   0.287 0.774162
# ------V1A12       -5.201e-01  2.681e-01  -1.940 0.052408 .
# ------V1A13       -1.150e+00  4.473e-01  -2.570 0.010173 *
# ------V1A14       -1.675e+00  2.750e-01  -6.091 1.12e-09 ***
# ------V2           2.570e-02  1.159e-02   2.217 0.026647 *
# ------V3A31        8.440e-02  6.580e-01   0.128 0.897943
# ------V3A32       -8.078e-01  4.996e-01  -1.617 0.105907
# ------V3A33       -7.683e-01  5.372e-01  -1.430 0.152634
# ------V3A34       -1.446e+00  5.127e-01  -2.821 0.004784 **
# ------V4A41       -1.513e+00  4.479e-01  -3.379 0.000728 ***
# ------V4A410      -2.412e+00  1.160e+00  -2.080 0.037543 *
# ------V4A42       -5.496e-01  3.195e-01  -1.720 0.085354 .
# ------V4A43       -9.142e-01  3.024e-01  -3.023 0.002503 **
# ------V4A44       -4.163e-01  9.455e-01  -0.440 0.659751
# ------V4A45       -1.562e-01  6.742e-01  -0.232 0.816732
# ------V4A46       -2.569e-01  5.085e-01  -0.505 0.613382
# ------V4A48       -1.531e+01  4.556e+02  -0.034 0.973202
# ------V4A49       -5.397e-01  4.017e-01  -1.344 0.179086
# ------V5           1.076e-04  5.600e-05   1.922 0.054633 .
# ------V6A62       -3.474e-01  3.579e-01  -0.971 0.331777
# ------V6A63       -2.440e-01  4.761e-01  -0.513 0.608232
# ------V6A64       -1.379e+00  6.535e-01  -2.110 0.034823 *
# ------V6A65       -8.106e-01  3.223e-01  -2.515 0.011910 *
# ------V7A72       -1.814e-01  5.243e-01  -0.346 0.729300
# ------V7A73       -5.253e-01  5.001e-01  -1.050 0.293529
# ------V7A74       -1.129e+00  5.455e-01  -2.070 0.038431 *
# ------V7A75       -5.927e-01  5.052e-01  -1.173 0.240705
# ------V8           3.523e-01  1.094e-01   3.219 0.001284 **
# ------V9A92        4.849e-02  4.760e-01   0.102 0.918863
# ------V9A93       -4.446e-01  4.691e-01  -0.948 0.343279
# ------V9A94       -4.288e-01  5.837e-01  -0.735 0.462524
# ------V10A102      3.052e-01  5.338e-01   0.572 0.567472
# ------V10A103     -3.086e-01  5.237e-01  -0.589 0.555669
# ------V11         -1.080e-01  1.073e-01  -1.007 0.314147
# ------V12A122      2.219e-01  3.161e-01   0.702 0.482767
# ------V12A123      3.274e-01  2.922e-01   1.120 0.262504
# ------V12A124      1.156e+00  5.656e-01   2.044 0.040944 *
# ------V13         -2.257e-02  1.140e-02  -1.980 0.047667 *
# ------V14A142     -5.214e-01  4.925e-01  -1.059 0.289757
# ------V14A143     -7.780e-01  2.848e-01  -2.732 0.006299 **
# ------V15A152     -6.323e-01  2.870e-01  -2.203 0.027579 *
# ------V15A153     -6.674e-01  6.202e-01  -1.076 0.281931
# ------V16          2.866e-01  2.236e-01   1.282 0.199939
# ------V17A172      1.565e+00  8.891e-01   1.760 0.078442 .
# ------V17A173      1.564e+00  8.582e-01   1.823 0.068370 .
# ------V17A174      1.400e+00  8.772e-01   1.596 0.110563
# ------V18          1.645e-01  3.004e-01   0.548 0.583871
# ------V19A192     -3.319e-01  2.413e-01  -1.376 0.168942
# ------V20A202     -2.137e+00  8.573e-01  -2.493 0.012665 *
# ---------
# ------Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

# ------(Dispersion parameter for binomial family taken to be 1)

# ------    Null deviance: 851.79  on 699  degrees of freedom
# ------Residual deviance: 613.21  on 651  degrees of freedom
# ------AIC: 711.21

# ------Number of Fisher Scoring iterations: 14

model1_predict <- predict(model1,newdata = data_test[,-21], type = "response")
t1 <- table(data_test$V21,as.integer(model1_predict>0.5))

acc1 <- (t1[1,1]+t1[2,2])/sum(t1)
loss1 <- t1[2,1]*5+t1[1,2]

#      0   1
#  0 183  25
#  1  48  44

## Re-run the model with significant variables (p <= 0.05)
## V1+V2+V3+V4+V6+V7+V8+V12+V13+V14+V15+V20

model2 <- glm(V21 ~ V1+V2+V3+V4+V6+V7+V8+V12+V13+V14+V15+V20,family = binomial(link = "logit"),data = data_train)
summary(model2)

# ------Call:
# ------glm(formula = V21 ~ V1 + V2 + V3 + V4 + V6 + V7 + V8 + V12 + 
# ------    V13 + V14 + V15 + V20, family = binomial(link = "logit"), 
# ------    data = data_train)

# ------Deviance Residuals: 
# ------    Min       1Q   Median       3Q      Max  
# -------2.1343  -0.7151  -0.3829   0.6860   2.4613  

# ------Coefficients:
# ------              Estimate Std. Error z value Pr(>|z|)    
# ------(Intercept)   1.954704   0.891687   2.192 0.028369 *  
# ------V1A12        -0.576472   0.261484  -2.205 0.027481 *  
# ------V1A13        -1.246843   0.431631  -2.889 0.003869 ** 
# ------V1A14        -1.633312   0.268529  -6.082 1.18e-09 ***
# ------V2            0.036245   0.009018   4.019 5.83e-05 ***
# ------V3A31        -0.077212   0.626544  -0.123 0.901921    
# ------V3A32        -0.971152   0.467517  -2.077 0.037778 *  
# ------V3A33        -0.743337   0.523613  -1.420 0.155715    
# ------V3A34        -1.447406   0.498458  -2.904 0.003687 ** 
# ------V4A41        -1.375418   0.415248  -3.312 0.000925 ***
# ------V4A410       -2.141135   1.005226  -2.130 0.033171 *  
# ------V4A42        -0.448810   0.306760  -1.463 0.143450    
# ------V4A43        -0.939346   0.291483  -3.223 0.001270 ** 
# ------V4A44        -0.492708   0.950608  -0.518 0.604243    
# ------V4A45        -0.238889   0.676434  -0.353 0.723969    
# ------V4A46        -0.268251   0.499637  -0.537 0.591342    
# ------V4A48       -15.257528 451.906122  -0.034 0.973066    
# ------V4A49        -0.570040   0.388370  -1.468 0.142165    
# ------V6A62        -0.307611   0.343036  -0.897 0.369862    
# ------V6A63        -0.484692   0.466898  -1.038 0.299218    
# ------V6A64        -1.212443   0.622420  -1.948 0.051421 .  
# ------V6A65        -0.810257   0.313011  -2.589 0.009637 ** 
# ------V7A72         0.199964   0.461824   0.433 0.665025    
# ------V7A73        -0.179579   0.428502  -0.419 0.675153    
# ------V7A74        -0.874730   0.481305  -1.817 0.069154 .  
# ------V7A75        -0.318585   0.442407  -0.720 0.471453    
# ------V8            0.235399   0.095022   2.477 0.013237 *  
# ------V12A122       0.282784   0.305057   0.927 0.353932    
# ------V12A123       0.433363   0.275990   1.570 0.116365    
# ------V12A124       1.052152   0.533417   1.972 0.048555 *  
# ------V13          -0.022929   0.010929  -2.098 0.035904 *  
# ------V14A142      -0.485528   0.484466  -1.002 0.316251    
# ------V14A143      -0.716748   0.278048  -2.578 0.009944 ** 
# ------V15A152      -0.592894   0.264661  -2.240 0.025078 *  
# ------V15A153      -0.512743   0.587424  -0.873 0.382735    
# ------V20A202      -2.057069   0.840733  -2.447 0.014415 *  
# ---------
# ------Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

# ------(Dispersion parameter for binomial family taken to be 1)

# ------    Null deviance: 851.79  on 699  degrees of freedom
# ------Residual deviance: 628.99  on 664  degrees of freedom
# ------AIC: 700.99

# ------Number of Fisher Scoring iterations: 14

model2_predict <- predict(model2,newdata = data_test[,-21], type = "response")
t2 <- table(data_test$V21,as.integer(model2_predict>0.5))

acc2 <- (t2[1,1]+t2[2,2])/sum(t2)
loss2 <- t2[2,1]*5+t2[1,2]

#     0   1
#  0 180  28
#  1  49  43

## Re-run the model with significant variables (p <= 0.05)
## V1+V2+V3+V4+V6+V8+V12+V13+V14+V15+V20
model3 <- glm(V21 ~ V1+V2+V3+V4+V6+V8+V12+V13+V14+V15+V20,family = binomial(link = "logit"),data = data_train)
summary(model3)

# ------Call:
# ------glm(formula = V21 ~ V1 + V2 + V3 + V4 + V6 + V8 + V12 + V13 + 
# ------    V14 + V15 + V20, family = binomial(link = "logit"), data = data_train)

# ------Deviance Residuals: 
# ------    Min       1Q   Median       3Q      Max  
# -------2.1140  -0.7309  -0.3992   0.7262   2.6620  

# ------Coefficients:
# ------              Estimate Std. Error z value Pr(>|z|)    
# ------(Intercept)   1.971739   0.746245   2.642 0.008237 ** 
# ------V1A12        -0.492608   0.254998  -1.932 0.053382 .  
# ------V1A13        -1.156215   0.424349  -2.725 0.006436 ** 
# ------V1A14        -1.605108   0.265374  -6.048 1.46e-09 ***
# ------V2            0.033550   0.008783   3.820 0.000134 ***
# ------V3A31        -0.182160   0.623567  -0.292 0.770190    
# ------V3A32        -1.052691   0.461930  -2.279 0.022673 *  
# ------V3A33        -0.801449   0.518845  -1.545 0.122424    
# ------V3A34        -1.570962   0.490694  -3.202 0.001367 ** 
# ------V4A41        -1.417883   0.414529  -3.420 0.000625 ***
# ------V4A410       -1.978036   0.980023  -2.018 0.043554 *  
# ------V4A42        -0.408540   0.300536  -1.359 0.174029    
# ------V4A43        -0.932034   0.287107  -3.246 0.001169 ** 
# ------V4A44        -0.394596   0.910382  -0.433 0.664695    
# ------V4A45        -0.146272   0.659895  -0.222 0.824579    
# ------V4A46        -0.211623   0.494183  -0.428 0.668485    
# ------V4A48       -15.306333 458.685224  -0.033 0.973380    
# ------V4A49        -0.610577   0.383411  -1.592 0.111275    
# ------V6A62        -0.369546   0.338713  -1.091 0.275260    
# ------V6A63        -0.494063   0.457275  -1.080 0.279942    
# ------V6A64        -1.207293   0.604626  -1.997 0.045851 *  
# ------V6A65        -0.879751   0.308183  -2.855 0.004309 ** 
# ------V8            0.223822   0.094210   2.376 0.017512 *  
# ------V12A122       0.302086   0.298543   1.012 0.311602    
# ------V12A123       0.401634   0.271930   1.477 0.139681    
# ------V12A124       1.083056   0.526907   2.055 0.039831 *  
# ------V13          -0.025963   0.010028  -2.589 0.009622 ** 
# ------V14A142      -0.400231   0.474423  -0.844 0.398884    
# ------V14A143      -0.714374   0.275354  -2.594 0.009476 ** 
# ------V15A152      -0.589884   0.261522  -2.256 0.024097 *  
# ------V15A153      -0.523675   0.584766  -0.896 0.370505    
# ------V20A202      -2.005458   0.831808  -2.411 0.015911 *  
# ---------
# ------Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

# ------(Dispersion parameter for binomial family taken to be 1)

# ------    Null deviance: 851.79  on 699  degrees of freedom
# ------Residual deviance: 638.92  on 668  degrees of freedom
# ------AIC: 702.92

# ------Number of Fisher Scoring iterations: 14

model3_predict <- predict(model3,newdata = data_test[,-21], type = "response")
t3 <- table(data_test$V21,as.integer(model3_predict>0.5))

acc3 <- (t3[1,1]+t3[2,2])/sum(t3)
loss3 <- t3[2,1]*5+t3[1,2]

#     0   1
#  0 179  29
#  1  52  40

## Lets select the categorical variables with significnat value (P <= 0.05)
## V1A13+V1A14+V2+V3A32+V3A34+V4A41+V4A410++V4A43+V6A64++V6A65+V8+V12A124+V13+V14A143+V15A152+V20A202
## Converting categorical variables in 0 and 1
data_train$V1A13[data_train$V1 == "A13"] <- 1
data_train$V1A13[data_train$V1 != "A13"] <- 0

data_train$V1A14[data_train$V1 == "A14"] <- 1
data_train$V1A14[data_train$V1 != "A14"] <- 0

data_train$V3A32[data_train$V3 == "A32"] <- 1
data_train$V3A32[data_train$V3 != "A32"] <- 0

data_train$V3A34[data_train$V3 == "A34"] <- 1
data_train$V3A34[data_train$V3 != "A34"] <- 0

data_train$V4A41[data_train$V4 == "A41"] <- 1
data_train$V4A41[data_train$V4 != "A41"] <- 0

data_train$V4A410[data_train$V4 == "A410"] <- 1
data_train$V4A410[data_train$V4 != "A410"] <- 0

data_train$V4A43[data_train$V4 == "A43"] <- 1
data_train$V4A43[data_train$V4 != "A43"] <- 0

data_train$V6A64[data_train$V6 == "A64"] <- 1
data_train$V6A64[data_train$V6 != "A64"] <- 0

data_train$V6A65[data_train$V6 == "A65"] <- 1
data_train$V6A65[data_train$V6 != "A65"] <- 0

data_train$V12A124[data_train$V12 == "A124"] <- 1
data_train$V12A124[data_train$V12 != "A124"] <- 0

data_train$V14A143[data_train$V14 == "A143"] <- 1
data_train$V14A143[data_train$V14 != "A143"] <- 0

data_train$V15A152[data_train$V15 == "A152"] <- 1
data_train$V15A152[data_train$V15 != "A152"] <- 0

data_train$V20A202[data_train$V20 == "A202"] <- 1
data_train$V20A202[data_train$V20 != "A202"] <- 0

## Run the model with above variables

model4 <- glm(V21 ~ V1A13+V1A14+V2+V3A32+V3A34+V4A41+V4A410+V4A43+V6A64++V6A65+V8+V12A124+V13+V14A143+V15A152+V20A202,family = binomial(link = "logit"),data = data_train)
summary(model4)

# ------Call:
# ------glm(formula = V21 ~ V1A13 + V1A14 + V2 + V3A32 + V3A34 + V4A41 + 
# ------    V4A410 + +V4A43 + V6A64 + +V6A65 + V8 + V12A124 + V13 + V14A143 + 
# ------    V15A152 + V20A202, family = binomial(link = "logit"), data = data_train)

# ------Deviance Residuals: 
# ------    Min       1Q   Median       3Q      Max  
# -------1.9557  -0.7539  -0.4199   0.7809   2.7526  

# ------Coefficients:
# ------             Estimate Std. Error z value Pr(>|z|)    
# ------(Intercept)  0.864532   0.536043   1.613  0.10679    
# ------V1A13       -0.875323   0.397246  -2.203  0.02756 *  
# ------V1A14       -1.445888   0.223973  -6.456 1.08e-10 ***
# ------V2           0.035060   0.008390   4.179 2.93e-05 ***
# ------V3A32       -0.383515   0.246022  -1.559  0.11903    
# ------V3A34       -0.907585   0.293624  -3.091  0.00199 ** 
# ------V4A41       -1.118973   0.378551  -2.956  0.00312 ** 
# ------V4A410      -1.969744   1.004093  -1.962  0.04980 *  
# ------V4A43       -0.718168   0.231034  -3.109  0.00188 ** 
# ------V6A64       -1.178612   0.590990  -1.994  0.04612 *  
# ------V6A65       -0.810575   0.289310  -2.802  0.00508 ** 
# ------V8           0.232289   0.090570   2.565  0.01033 *  
# ------V12A124      0.479372   0.326934   1.466  0.14258    
# ------V13         -0.025587   0.009497  -2.694  0.00705 ** 
# ------V14A143     -0.604435   0.234291  -2.580  0.00988 ** 
# ------V15A152     -0.615322   0.239552  -2.569  0.01021 *  
# ------V20A202     -1.558080   0.781280  -1.994  0.04612 *  
# ---------
# ------Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

# ------(Dispersion parameter for binomial family taken to be 1)

# ------    Null deviance: 851.79  on 699  degrees of freedom
# ------Residual deviance: 660.51  on 683  degrees of freedom
# ------AIC: 694.51

# ------Number of Fisher Scoring iterations: 5

## Converting categorical variables in 0 and 1
data_test$V1A13[data_test$V1 == "A13"] <- 1
data_test$V1A13[data_test$V1 != "A13"] <- 0

data_test$V1A14[data_test$V1 == "A14"] <- 1
data_test$V1A14[data_test$V1 != "A14"] <- 0

data_test$V3A32[data_test$V3 == "A32"] <- 1
data_test$V3A32[data_test$V3 != "A32"] <- 0

data_test$V3A34[data_test$V3 == "A34"] <- 1
data_test$V3A34[data_test$V3 != "A34"] <- 0

data_test$V4A41[data_test$V4 == "A41"] <- 1
data_test$V4A41[data_test$V4 != "A41"] <- 0

data_test$V4A410[data_test$V4 == "A410"] <- 1
data_test$V4A410[data_test$V4 != "A410"] <- 0

data_test$V4A43[data_test$V4 == "A43"] <- 1
data_test$V4A43[data_test$V4 != "A43"] <- 0

data_test$V6A64[data_test$V6 == "A64"] <- 1
data_test$V6A64[data_test$V6 != "A64"] <- 0

data_test$V6A65[data_test$V6 == "A65"] <- 1
data_test$V6A65[data_test$V6 != "A65"] <- 0

data_test$V12A124[data_test$V12 == "A124"] <- 1
data_test$V12A124[data_test$V12 != "A124"] <- 0

data_test$V14A143[data_test$V14 == "A143"] <- 1
data_test$V14A143[data_test$V14 != "A143"] <- 0

data_test$V15A152[data_test$V15 == "A152"] <- 1
data_test$V15A152[data_test$V15 != "A152"] <- 0

data_test$V20A202[data_test$V20 == "A202"] <- 1
data_test$V20A202[data_test$V20 != "A202"] <- 0

model4_predict <- predict(model4,newdata = data_test[,-21], type = "response")
t4 <- table(data_test$V21,as.integer(model4_predict>0.5))

acc4 <- (t4[1,1]+t4[2,2])/sum(t4)
loss4 <- t4[2,1]*5+t4[1,2]

#      0   1
#  0 177  31
#  1  50  42

## Removing the V3A32 and V12A124 categorical variables with p-value > 0.05 and re-run the model

## Run the model with above variables

model5 <- glm(V21 ~ V1A13+V1A14+V2+V3A34+V4A41+V4A410+V4A43+V6A64++V6A65+V8+V13+V14A143+V15A152+V20A202,family = binomial(link = "logit"),data = data_train)
summary(model5)

# ------Call: -->
# ------glm(formula = V21 ~ V1A13 + V1A14 + V2 + V3A34 + V4A41 + V4A410 +  -->
# ------    V4A43 + V6A64 + +V6A65 + V8 + V13 + V14A143 + V15A152 + V20A202,  -->
# ------    family = binomial(link = "logit"), data = data_train) -->

# ------Deviance Residuals:  -->
# ------    Min       1Q   Median       3Q      Max   -->
# -------1.9225  -0.7645  -0.4100   0.8081   2.7171   -->

# ------Coefficients: -->
# ------             Estimate Std. Error z value Pr(>|z|)     -->
# ------(Intercept)  0.591230   0.504195   1.173 0.240948     -->
# ------V1A13       -0.883623   0.397352  -2.224 0.026164 *   -->
# ------V1A14       -1.452682   0.223689  -6.494 8.35e-11 *** -->
# ------V2           0.039023   0.008156   4.785 1.71e-06 *** -->
# ------V3A34       -0.619741   0.237625  -2.608 0.009106 **  -->
# ------V4A41       -1.120728   0.375451  -2.985 0.002836 **  -->
# ------V4A410      -1.827306   0.970627  -1.883 0.059754 .   -->
# ------V4A43       -0.768113   0.229202  -3.351 0.000804 *** -->
# ------V6A64       -1.217879   0.592261  -2.056 0.039752 *   -->
# ------V6A65       -0.818100   0.286878  -2.852 0.004348 **  -->
# ------V8           0.238850   0.089660   2.664 0.007723 **  -->
# ------V13         -0.021183   0.009001  -2.353 0.018601 *   -->
# ------V14A143     -0.684019   0.229249  -2.984 0.002848 **  -->
# ------V15A152     -0.777706   0.209086  -3.720 0.000200 *** -->
# ------V20A202     -1.581760   0.784418  -2.016 0.043750 *   -->
# --------- -->
# ------Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 -->

# ------(Dispersion parameter for binomial family taken to be 1) -->

# ------    Null deviance: 851.79  on 699  degrees of freedom -->
# ------Residual deviance: 665.25  on 685  degrees of freedom -->
# ------AIC: 695.25 -->

# ------Number of Fisher Scoring iterations: 5 -->

model5_predict <- predict(model5,newdata = data_test[,-21], type = "response")
t5 <- table(data_test$V21,as.integer(model5_predict>0.5))

acc5 <- (t5[1,1]+t5[2,2])/sum(t5)
loss5 <- t5[2,1]*5+t5[1,2]

#     0   1
#  0 173  35
#  1  52  40

## Removing the V4A410 with p-value > 0.05 and re-run the model

model6 <- glm(V21 ~ V1A13+V1A14+V2+V3A34+V4A41+V4A43+V6A64++V6A65+V8+V13+V14A143+V15A152+V20A202,family = binomial(link = "logit"),data = data_train)
summary(model6)

# ------Call: -->
# ------glm(formula = V21 ~ V1A13 + V1A14 + V2 + V3A34 + V4A41 + V4A43 +  -->
# ------    V6A64 + +V6A65 + V8 + V13 + V14A143 + V15A152 + V20A202,  -->
# ------    family = binomial(link = "logit"), data = data_train) -->

# ------Deviance Residuals:  -->
# ------    Min       1Q   Median       3Q      Max   -->
# -------1.8572  -0.7647  -0.4133   0.8228   2.7176   -->

# ------Coefficients: -->
# ------             Estimate Std. Error z value Pr(>|z|)     -->
# ------(Intercept)  0.604293   0.502071   1.204 0.228744     -->
# ------V1A13       -0.857778   0.396501  -2.163 0.030513 *   -->
# ------V1A14       -1.417346   0.222161  -6.380 1.77e-10 *** -->
# ------V2           0.036347   0.007990   4.549 5.39e-06 *** -->
# ------V3A34       -0.606238   0.236529  -2.563 0.010375 *   -->
# ------V4A41       -1.057766   0.371945  -2.844 0.004457 **  -->
# ------V4A43       -0.743271   0.228114  -3.258 0.001121 **  -->
# ------V6A64       -1.199791   0.590430  -2.032 0.042147 *   -->
# ------V6A65       -0.815065   0.287363  -2.836 0.004563 **  -->
# ------V8           0.245521   0.089275   2.750 0.005957 **  -->
# ------V13         -0.023079   0.008962  -2.575 0.010022 *   -->
# ------V14A143     -0.648720   0.227909  -2.846 0.004422 **  -->
# ------V15A152     -0.749786   0.207559  -3.612 0.000303 *** -->
# ------V20A202     -1.739771   0.790105  -2.202 0.027669 *   -->
# --------- -->
# ------Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 -->

# ------(Dispersion parameter for binomial family taken to be 1) -->

# ------    Null deviance: 851.79  on 699  degrees of freedom -->
# ------Residual deviance: 669.23  on 686  degrees of freedom -->
# ------AIC: 697.23 -->

# ------Number of Fisher Scoring iterations: 5 -->

model6_predict <- predict(model6,newdata = data_test[,-21], type = "response")
t6 <- table(data_test$V21,as.integer(model6_predict>0.5))

acc6 <- (t6[1,1]+t6[2,2])/sum(t6)
loss6 <- t6[2,1]*5+t6[1,2]

#      0   1
#  0 175  33
#  1  51  41

## V1A14+V2+V4A41+V4A43+V6A65+V8+V14A143+V15A152 (p-value < 0.01)
model7 <- glm(V21 ~ V1A14+V2+V4A41+V4A43+V6A65+V8+V14A143+V15A152,family = binomial(link = "logit"),data = data_train)
summary(model7)

# -------Call: -->
# -------glm(formula = V21 ~ V1A14 + V2 + V4A41 + V4A43 + V6A65 + V8 +  -->
# -------    V14A143 + V15A152, family = binomial(link = "logit"), data = data_train) -->

# -------Deviance Residuals:  -->
# -------    Min       1Q   Median       3Q      Max   -->
# --------1.9248  -0.7971  -0.4810   0.9103   2.6646   -->

# -------Coefficients: -->
# -------             Estimate Std. Error z value Pr(>|z|)     -->
# -------(Intercept) -0.589625   0.390666  -1.509 0.131226     -->
# -------V1A14       -1.397374   0.215155  -6.495 8.32e-11 *** -->
# -------V2           0.042060   0.007802   5.391 7.01e-08 *** -->
# -------V4A41       -1.125054   0.372372  -3.021 0.002517 **  -->
# -------V4A43       -0.652980   0.220204  -2.965 0.003024 **  -->
# -------V6A65       -0.785492   0.276859  -2.837 0.004552 **  -->
# -------V8           0.205956   0.086263   2.388 0.016962 *   -->
# -------V14A143     -0.571174   0.222013  -2.573 0.010091 *   -->
# -------V15A152     -0.739550   0.200980  -3.680 0.000233 *** -->
# ---------- -->
# -------Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 -->

# -------(Dispersion parameter for binomial family taken to be 1) -->

# -------    Null deviance: 851.79  on 699  degrees of freedom -->
# -------Residual deviance: 703.27  on 691  degrees of freedom -->
# -------AIC: 721.27 -->

# -------Number of Fisher Scoring iterations: 5 -->

model7_predict <- predict(model7,newdata = data_test[,-21], type = "response")
t7 <- table(data_test$V21,as.integer(model7_predict>0.5))

acc7 <- (t7[1,1]+t7[2,2])/sum(t7)
loss7 <- t7[2,1]*5+t7[1,2]

#      0   1
#  0 183  25
#  1  58  34

## V1A14+V2+V4A41+V4A43+V6A65+V15A152 (p-value < 0.01)
model8 <- glm(V21 ~ V1A14+V2+V4A41+V4A43+V6A65+V15A152,family = binomial(link = "logit"),data = data_train)
summary(model8)

# -------Call: -->
# -------glm(formula = V21 ~ V1A14 + V2 + V4A41 + V4A43 + V6A65 + V15A152,  -->
# -------    family = binomial(link = "logit"), data = data_train) -->

# -------Deviance Residuals:  -->
# -------    Min       1Q   Median       3Q      Max   -->
# --------1.8822  -0.8172  -0.4910   0.9663   2.6170   -->

# -------Coefficients: -->
# -------             Estimate Std. Error z value Pr(>|z|)     -->
# -------(Intercept) -0.498742   0.239304  -2.084 0.037148 *   -->
# -------V1A14       -1.377923   0.212798  -6.475 9.47e-11 *** -->
# -------V2           0.043411   0.007668   5.662 1.50e-08 *** -->
# -------V4A41       -1.163712   0.365807  -3.181 0.001467 **  -->
# -------V4A43       -0.603378   0.216644  -2.785 0.005351 **  -->
# -------V6A65       -0.733093   0.272394  -2.691 0.007118 **  -->
# -------V15A152     -0.698937   0.197695  -3.535 0.000407 *** -->
# ---------- -->
# -------Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 -->

# -------(Dispersion parameter for binomial family taken to be 1) -->

# -------    Null deviance: 851.79  on 699  degrees of freedom -->
# -------Residual deviance: 715.84  on 693  degrees of freedom -->
# -------AIC: 729.84 -->

# -------Number of Fisher Scoring iterations: 4 -->

model8_predict <- predict(model8,newdata = data_test[,-21], type = "response")
t8 <- table(data_test$V21,as.integer(model8_predict>0.5))

acc8 <- (t8[1,1]+t8[2,2])/sum(t8)
loss8 <- t8[2,1]*5+t8[1,2]

#      0   1
#  0 178  30
#  1  58  34

#Each Iterations Accuracy
iterations <- c(seq(1,8))
accuracy <- c(acc1,acc2,acc3,acc4,acc5,acc6,acc7,acc8)
AIC <- c(model1$aic,model2$aic,model3$aic,model4$aic,model5$aic,model6$aic,model7$aic,model8$aic)
var_count <- c(length(model1$coefficients),length(model2$coefficients),length(model3$coefficients),length(model4$coefficients),length(model5$coefficients),length(model6$coefficients),length(model7$coefficients),length(model8$coefficients))
Loss <- c(loss1,loss2,loss3,loss4,loss5,loss6,loss7,loss8)

df <- data.frame(iterations, accuracy, AIC, var_count,Loss)
# Save results into a text file
sink("Models.txt")
print(df)
sink()

# plot ROC curve
library(pROC)

ROC1 <- roc(data_test$V21,as.integer(model1_predict>0.5))
ROC2 <- roc(data_test$V21,as.integer(model2_predict>0.5))
ROC3 <- roc(data_test$V21,as.integer(model3_predict>0.5))
ROC4 <- roc(data_test$V21,as.integer(model4_predict>0.5))
ROC5 <- roc(data_test$V21,as.integer(model5_predict>0.5))
ROC6 <- roc(data_test$V21,as.integer(model6_predict>0.5))
ROC7 <- roc(data_test$V21,as.integer(model7_predict>0.5))
ROC8 <- roc(data_test$V21,as.integer(model8_predict>0.5))

library(randomcoloR)
palette <- distinctColorPalette(8)
plot(ROC1,col = palette[1])
par(new=TRUE)
plot(ROC2,col = palette[2])
par(new=TRUE)
plot(ROC3,col = palette[3])
par(new=TRUE)
plot(ROC4,col = palette[4])
par(new=TRUE)
plot(ROC5,col = palette[5])
par(new=TRUE)
plot(ROC6,col = palette[6])
par(new=TRUE)
plot(ROC7,col = palette[7])
par(new=TRUE)
plot(ROC8,col = palette[8])

legend(0.4,0.5,legend=c(paste("Iteration 1 ",toString(round(ROC1$auc*100,0)),"%",sep=""),paste("Iteration 2 ",toString(round(ROC2$auc*100,0)),"%",sep=""),paste("Iteration 3 ",toString(round(ROC3$auc*100,0)),"%",sep=""),paste("Iteration 4 ",toString(round(ROC4$auc*100,0)),"%",sep=""),paste("Iteration 5 ",toString(round(ROC5$auc*100,0)),"%",sep=""),paste("Iteration 6 ",toString(round(ROC6$auc*100,0)),"%",sep=""),paste("Iteration 7 ",toString(round(ROC7$auc*100,0)),"%",sep=""),paste("Iteration 8 ",toString(round(ROC8$auc*100,0)),"%",sep="")),col=c(palette), lty=c(1,1), ncol=1)


# Lets look at the accuracy, auc and loss values for different threshold values for Model 4
acc <- c()
auc <- c()
loss <- c()

for(i in 1:100)
{
t <- as.matrix(table(data_test$V21,as.integer(model6_predict>(i/100))))
if(nrow(t)>1){FP <- t[2,1] }
if(ncol(t)>1){FN <- t[1,2] } else { FN <- 0 }
loss <- c(loss,FP*5+FN)
if(ncol(t)>1){acc <- c(acc,(t[1,1]+t[2,2])/sum(t)) } else { acc <- c(acc,(t[1,1])/sum(t)) }
ROC <- roc(data_test$V21,as.integer(model6_predict>(i/100)))
auc <- c(auc,ROC$auc)
}

plot(c(1:100)/100,loss,xlab = "Threshold", ylab = "Loss")
lines(c(1:100)/100,loss,xlab = "Threshold", ylab = "Loss",col = "red")

which.min(loss)

plot(c(1:100)/100,acc,xlab = "Threshold", ylab = "accuracy")
lines(c(1:100)/100,acc,xlab = "Threshold", ylab = "accuracy",col = "green")

which.max(acc)

plot(c(1:100)/100,auc,xlab = "Threshold", ylab = "AUC")
lines(c(1:100)/100,auc,xlab = "Threshold", ylab = "AUC",col = "blue")

which.max(auc)

# Save results into a text file
df <- data.frame(acc,auc,loss)
sink("Model6.txt")
print(df)
sink()




